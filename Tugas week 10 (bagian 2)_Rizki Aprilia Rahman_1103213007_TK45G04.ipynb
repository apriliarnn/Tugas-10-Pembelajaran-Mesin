{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjPmI7ApXCud",
        "outputId": "3c56402f-6296-4066-b838-281eb67d6cc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiments completed. Results saved to 'optimized_classification_results.csv'.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "file_path = 'winequality-red.csv'\n",
        "data = pd.read_csv(file_path, delimiter=';')\n",
        "\n",
        "# Separate features and target\n",
        "X = data.drop(columns=[\"quality\"]).values\n",
        "y = data[\"quality\"].values\n",
        "\n",
        "# Encode target labels for classification\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Create DataLoader objects\n",
        "def create_dataloader(X, y, batch_size):\n",
        "    dataset = TensorDataset(X, y)\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Define MLP model for classification\n",
        "class MLPClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_layers, activation_fn, num_classes):\n",
        "        super(MLPClassifier, self).__init__()\n",
        "        layers = []\n",
        "        current_size = input_size\n",
        "        for hidden_neurons in hidden_layers:\n",
        "            layers.append(nn.Linear(current_size, hidden_neurons))\n",
        "            layers.append(activation_fn)\n",
        "            current_size = hidden_neurons\n",
        "        layers.append(nn.Linear(current_size, num_classes))  # Output layer for classification\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Optimized Hyperparameters\n",
        "hidden_layer_configs = [[4], [16, 16]]\n",
        "activation_functions = {\n",
        "    \"relu\": nn.ReLU(),\n",
        "    \"tanh\": nn.Tanh()\n",
        "}\n",
        "epochs_list = [10, 25]\n",
        "learning_rates = [0.01, 0.001]\n",
        "batch_sizes = [64, 128]\n",
        "\n",
        "# Early Stopping Threshold\n",
        "early_stopping_threshold = 0.001\n",
        "\n",
        "def train_and_evaluate(\n",
        "    hidden_layers, activation_fn, epochs, learning_rate, batch_size\n",
        "):\n",
        "    train_loader = create_dataloader(X_train_tensor, y_train_tensor, batch_size)\n",
        "    test_loader = create_dataloader(X_test_tensor, y_test_tensor, batch_size)\n",
        "\n",
        "    model = MLPClassifier(X_train_tensor.shape[1], hidden_layers, activation_fn, len(label_encoder.classes_))\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Use GPU if available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        total_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_X, batch_y in test_loader:\n",
        "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "                outputs = model(batch_X)\n",
        "                loss = criterion(outputs, batch_y)\n",
        "                total_loss += loss.item() * batch_X.size(0)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total += batch_y.size(0)\n",
        "                correct += (predicted == batch_y).sum().item()\n",
        "\n",
        "        avg_loss = total_loss / len(test_loader.dataset)\n",
        "        accuracy = correct / total\n",
        "\n",
        "        if abs(best_loss - avg_loss) < early_stopping_threshold:\n",
        "            break\n",
        "        best_loss = avg_loss\n",
        "\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# Run experiments and collect results\n",
        "results = []\n",
        "for hidden_layers in hidden_layer_configs:\n",
        "    for activation_name, activation_fn in activation_functions.items():\n",
        "        for epochs in epochs_list:\n",
        "            for learning_rate in learning_rates:\n",
        "                for batch_size in batch_sizes:\n",
        "                    avg_loss, accuracy = train_and_evaluate(\n",
        "                        hidden_layers, activation_fn, epochs, learning_rate, batch_size\n",
        "                    )\n",
        "                    results.append(\n",
        "                        {\n",
        "                            \"hidden_layers\": hidden_layers,\n",
        "                            \"activation\": activation_name,\n",
        "                            \"epochs\": epochs,\n",
        "                            \"learning_rate\": learning_rate,\n",
        "                            \"batch_size\": batch_size,\n",
        "                            \"avg_loss\": avg_loss,\n",
        "                            \"accuracy\": accuracy,\n",
        "                        }\n",
        "                    )\n",
        "\n",
        "# Convert results to DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv(\"optimized_classification_results.csv\", index=False)\n",
        "print(\"Experiments completed. Results saved to 'optimized_classification_results.csv'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KMzzhnKVZdO5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}